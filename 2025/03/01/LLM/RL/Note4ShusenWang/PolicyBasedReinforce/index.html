<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Rayen Tian">





<title>Policy Based Reinforce | Rayen&#39;s Stories</title>



    <link rel="icon" href="/image/library.svg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Tawanna</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/tags">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Tawanna</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/tags">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Policy Based Reinforce</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Rayen Tian</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">三月 1, 2025&nbsp;&nbsp;14:34:25</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="Policy-Based-Reinforce"><a href="#Policy-Based-Reinforce" class="headerlink" title="Policy Based Reinforce"></a>Policy Based Reinforce</h1><h2 id="Policy-Network-pi-a-s-theta"><a href="#Policy-Network-pi-a-s-theta" class="headerlink" title="Policy Network $\pi(a|s;\theta)$"></a>Policy Network $\pi(a|s;\theta)$</h2><p>Policy Network: Use a neural net to approximate $\pi(a|s)$. </p>
<ul>
<li>Use policy network $\pi(a|s;\mathbf{\theta})$ to approximate $\pi(a|s)$.</li>
<li>$\theta$: trainable parameters of the neural net.</li>
<li>$\Sigma_{a\in A} \pi(a|s;\theta) = 1$ </li>
</ul>
<h2 id="Policy-Based-Reinforcement-Learning"><a href="#Policy-Based-Reinforcement-Learning" class="headerlink" title="Policy-Based Reinforcement Learning"></a>Policy-Based Reinforcement Learning</h2><p>==State-value function==: how good the situation is in state $s$</p>
<ul>
<li><p>$V\pi(s<em>t) = \mathbb{E}_A[Q</em>\pi(s<em>t,A)] = \Sigma_a \pi(a|s_t) \cdot Q</em>\pi(s_t,a).$</p>
</li>
<li><p>$V\pi(s<em>t) = \mathbb{E}_A[Q</em>\pi(s<em>t,A)] = \int \pi (a|s_t)\cdot Q</em>\pi(s_t,a) \,da.$ </p>
</li>
</ul>
<p>Approximate state-value function:</p>
<ul>
<li>Approximate policy function $\pi(a|s_t)$ by policy network $\pi(a|s_t;\theta)$.</li>
<li>Appproximate value function $V<em>\pi(s_t)$ by: $V\pi(s_t) = \Sigma_a \pi(a|s_t;\theta) \cdot Q</em>\pi(s_t,a).$ </li>
</ul>
<script type="math/tex; mode=display">
V(s;\theta) = \Sigma_a \pi(a|s;\theta) \cdot Q_\pi(s,a).</script><p>==Policy-Based learning==: Learn $\theta$ that maximizes $J(\theta) = \mathbb{E}_S[V(S;\theta)]$. </p>
<h2 id="Policy-gradient-ascent"><a href="#Policy-gradient-ascent" class="headerlink" title="Policy gradient ascent"></a>Policy gradient ascent</h2><ul>
<li>Observe state s.</li>
<li>Update policy by: $\theta \leftarrow \theta + \beta \cdot \frac{\partial V(s;\theta)}{\partial \theta}$</li>
</ul>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><script type="math/tex; mode=display">
\begin{align}
\frac{\partial V(s;\theta)}{\partial \theta} &= \frac{ \Sigma_a \partial \pi(a|s;\theta) \cdot Q_\pi(s,a) }{\partial \theta} \\
&=\Sigma_a  \frac{ \partial \pi(a|s;\theta) }{\partial \theta}\cdot Q_\pi(s,a) \quad (Pretend \  Q_\pi \  is \  independent \  of \  \theta)\\
&= \Sigma_a \pi(a|s;\theta) \cdot \frac{ \partial log \pi(a|s;\theta) }{\partial \theta} \cdot Q_\pi(s,a) \\
&= \mathbb{E}_A[\frac{ \partial log \pi(a|s;\theta) }{\partial \theta} \cdot Q_\pi(s,A) ]
\end{align}</script><h3 id="Calculate-Policy-Gradient-for-Discrete-Actions"><a href="#Calculate-Policy-Gradient-for-Discrete-Actions" class="headerlink" title="Calculate Policy Gradient for Discrete Actions"></a>Calculate Policy Gradient for Discrete Actions</h3><ol>
<li><p>If the actions are ==discrete==, e.g. action space $\mathcal{A}$ = {“left”,”right”,”up”},…</p>
<p>Use $\frac{\partial V(s;\theta)}{\partial \theta} = \Sigma<em>a  \frac{ \partial \pi(a|s;\theta) }{\partial \theta}\cdot Q</em>\pi(s,a)$ </p>
<ol>
<li>Calculate $f(a,\theta) =  \frac{ \partial \pi(a|s;\theta) }{\partial \theta}\cdot Q_\pi(s,a) $ for every action $a \in \mathcal{A}$. </li>
<li>Policy gradient: $\frac{\partial V(s;\theta)}{\partial \theta} = f(left,\theta) + f(right,\theta) + f(up,\theta)$</li>
</ol>
</li>
<li><p>If the actions are ==continuous==, e.g. action space $\mathcal{A}$ = [0,1]. </p>
<p>Use $\frac{\partial V(s;\theta)}{\partial \theta} = \mathbb{E}<em>{A\sim\pi(\cdot|s;\theta)}[\frac{ \partial log \pi(a|s;\theta) }{\partial \theta} \cdot Q</em>\pi(s,A) ]$  ==It’s difficult to calculate the $\int$ of $\pi$. So we use Monte Carlo Approximation.==</p>
<ol>
<li>Randomly sample an action $\hat{a}$ according to the PDF $\pi(\cdot | s;\theta)$ </li>
<li>Calculate $g(\hat{a},\theta) = \frac{ \partial log \pi(\hat{a}|s;\theta) }{\partial \theta} \cdot Q_\pi(s,\hat{a})$. </li>
</ol>
<blockquote>
<p>[!NOTE]</p>
<p>Obviously, $\mathbb{E}_A[g(A,\theta)] = \frac{\partial V(s;\theta)}{\partial \theta}$.</p>
<p>$g(\hat{a},\theta)$ is an unbiased estimate of $\frac{\partial V(s;\theta)}{\partial \theta}$.</p>
</blockquote>
<ol>
<li>Use $g(\hat{a},\theta)$ as an approximation to the policy gradient $\frac{\partial V(s;\theta)}{\partial \theta}$. (This is called Monte Carlo approximation. The unbiased estimate is important.) </li>
</ol>
<blockquote>
<p>[!IMPORTANT]</p>
<p>This approach also works for ==discrete== actions.</p>
</blockquote>
</li>
</ol>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ol>
<li>Observe the state $s_t$.</li>
<li>Randomly sample action $a_t$ according to $\pi(\cdot | s_t;\theta_t)$ </li>
<li>Compute $q<em>t \approx Q</em>\pi(s_t,a_t)$ (some estimate).</li>
<li>Differentiate policy network: $\mathbf{d}<em>{\theta,t} = \frac{\partial log \ \pi(a_t | s_t, \theta)}{\partial \theta} |</em>{\theta = \theta_t}$</li>
<li>(Approximate) policy gradient: $\mathbf{g}(\hat{a},\theta) = q<em>t \cdot \mathbf{d</em>{\theta,t}}$ .</li>
<li>Update policy network: $\theta_{t+1} = \theta_t + \beta \cdot \mathbf{g}(a_t, \theta_t)$.</li>
</ol>
<h3 id="Compuet-qt-approx-Q-pi-s-t-a-t"><a href="#Compuet-qt-approx-Q-pi-s-t-a-t" class="headerlink" title="Compuet  $qt \approx Q\pi(s_t,a_t)$"></a>Compuet  $q<em>t \approx Q</em>\pi(s_t,a_t)$</h3><ol>
<li><p>Option 1: REINFORCE</p>
<ol>
<li>Play the game to the end and generate the trajectory: $s_1,a_1,r_1,s_2,a_2,r_2,…,s_T,a_T,r_T$.</li>
<li>Compute the discounted return $u<em>t = \Sigma</em>{k = t}^T \gamma^{k-t}r_k$, for all t.</li>
<li>Since $Q<em>\pi(s_t,a_t) = \mathbb{E}[U_t]$, we can use $u_t$ to approximate $Q</em>\pi(s_t,a_t)$.</li>
<li>$\rightarrow$ Use $q_t = u_t$</li>
</ol>
</li>
<li><p>Option 2: Approximate $Q_\pi$ using a neural network.</p>
<p>This lead to the actor-critic method.</p>
</li>
</ol>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Rayen Tian</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://rayentian.github.io/2025/03/01/LLM/RL/Note4ShusenWang/PolicyBasedReinforce/">https://rayentian.github.io/2025/03/01/LLM/RL/Note4ShusenWang/PolicyBasedReinforce/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>I just want something simpler...</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/LLM/"># LLM</a>
                    
                        <a href="/tags/RL/"># RL</a>
                    
                        <a href="/tags/ShusenWang/"># ShusenWang</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2025/03/01/LLM/RL/Note4ShusenWang/ValueBasedReinforce/">Value Based Reinforce</a>
            
            
            <a class="next" rel="next" href="/2025/03/01/LLM/RL/Note4ShusenWang/BasicConcept/">Basic Concept</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Rayen Tian | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>